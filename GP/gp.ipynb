{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run various query selection methods and evaluate performance using a Gaussian Process Model\n",
    "Methods implemented:\n",
    "- Mean Acquisition\n",
    "- Variance Acquisition\n",
    "- Upper Confidence Bound Acquisition\n",
    "- Biased Upper Confidence Bound Acquisition\n",
    "- Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data and subsetting columns\n",
    "data = pd.read_csv(\"TYK2_final.csv\")\n",
    "data = data.drop(['target', 'top_2p'], axis=1)\n",
    "column_names = ['smiles', 'target', 'active']\n",
    "data.columns = column_names\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to fingeraprints and binarizing the \"active\" column\n",
    "def smiles_to_fingerprint(smiles, nBits=4096):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=4, nBits=nBits, useChirality=True)\n",
    "    return list(fp)\n",
    "\n",
    "data['fingerprint'] = data['smiles'].apply(smiles_to_fingerprint)\n",
    "\n",
    "data['active'] = data['active'].replace({False: 0, True: 1})\n",
    "\n",
    "data.to_csv(\"TYK2_fingerprints.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform random sampling, we will randomly select 1,500 of the instances from the dataset to train on, and evaluate on the remaining ligands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function runs random sampling based on the \"batch size\" (which isn't relevant for random selection)s\n",
    "#for each seed, we randomly start with 1,000 ligands to train on. While the training set isn't 1,500 ligands yet,\n",
    "#we get the performance (RMSE, R^2) if the counter is at a multiple of 50,\n",
    "#and then append new batch size set of data to the training data\n",
    "#At the end of the seed, we append the list of RMSE and R^2 values to the master list\n",
    "def random_sampling(batch_size):\n",
    "\n",
    "    #setting up initial seeds\n",
    "    seeds = [1, 2, 3]\n",
    "\n",
    "    #storing master list of RMSE and R^2 values\n",
    "    rmse_vals = []\n",
    "    r2_vals = []\n",
    "  \n",
    "    #for each seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        #storing indices of data points selected (for UMAP analysis)\n",
    "        final_inds = []\n",
    "\n",
    "        #storing RMSE and R^2 values every 50 instances added\n",
    "        rmse_instance =[]\n",
    "        r2_instance = []\n",
    "\n",
    "        #getting data (without the smiles column)\n",
    "        working_data = data.drop(columns=\"smiles\")\n",
    "        #setting starting size\n",
    "        size = 1000\n",
    "        #sampling 'size' amount of data points\n",
    "        start_data = working_data.sample(n=size, random_state=seed)\n",
    "        \n",
    "        #Extracting starting rows (X and labels)\n",
    "        start_X = np.array([val for val in start_data['fingerprint'].values])\n",
    "        start_y = start_data['target'].values.reshape(-1,1)\n",
    "        start_classes = start_data['active'].values.reshape(-1,1)\n",
    "\n",
    "        #Extracting remaining rows (X and labels)\n",
    "        remaining_data = working_data.drop(start_data.index)\n",
    "        remaining_X = np.array([val for val in remaining_data['fingerprint'].values])\n",
    "        remaining_y = remaining_data['target'].values.reshape(-1,1)\n",
    "        remaining_classes = remaining_data['active'].values.reshape(-1,1)\n",
    "        \n",
    "        #setting counter to 0\n",
    "        counter = 0\n",
    "\n",
    "        #for as long as our training set is 1,500 ligands\n",
    "        while len(start_X) <= 1500:\n",
    "\n",
    "            #record RMSE and R^2 every 50 instances added\n",
    "            if counter % 50 == 0:\n",
    "                #creating kernel\n",
    "                k = GPy.kern.Linear(len(start_X[0]))\n",
    "\n",
    "                #training and optimizing GP regression model\n",
    "                m = GPy.models.GPRegression(start_X, start_y, k)\n",
    "               \n",
    "                #Predicting on final 20%\n",
    "                pred_means, pred_vars = m.predict(remaining_X)\n",
    "\n",
    "                #Getting rmse and r^2 score\n",
    "                rmse_instance.append(np.sqrt(mean_squared_error(remaining_y, pred_means)))\n",
    "                r2_instance.append(r2_score(remaining_y, pred_means))\n",
    "            \n",
    "            #once our training size is 1,500, we exit the loop\n",
    "            if len(start_X) >= 1500:\n",
    "                break\n",
    "        \n",
    "            #sampling batch size random row(s) and saving indices for UMAP\n",
    "            rand_rows = remaining_data.sample(n=batch_size, random_state=seed)\n",
    "            final_inds.append(rand_rows.index.to_list())\n",
    "\n",
    "            #adding the row of the selected index to the starting data\n",
    "            start_data = pd.concat([start_data, rand_rows])\n",
    "            start_X = np.array([val for val in start_data['fingerprint'].values])\n",
    "            start_y = start_data['target'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "            #removing the row of the selected index from the remaining data\n",
    "            remaining_data = working_data.drop(start_data.index)\n",
    "            remaining_X = np.array([val for val in remaining_data['fingerprint'].values])\n",
    "            remaining_y = remaining_data['target'].values.reshape(-1,1)\n",
    "\n",
    "            #incrementing counter by batch size\n",
    "            counter += batch_size\n",
    "        \n",
    "        #append the current seed's list of values to the master list\n",
    "        rmse_vals.append(rmse_instance)\n",
    "        r2_vals.append(r2_instance)\n",
    "        \n",
    "    #return master lists and list of indices (just from seed 3)\n",
    "    return rmse_vals, r2_vals, final_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell runs the random sampling simulations for varying batch sizes (1, 25, and 50), and saves the RMSE and R^2 values, as well as the runtimes\n",
    "#The results are saved into a dictionary for each batch\n",
    "batch_sizes = [1, 25, 50]\n",
    "rand_rmse_dict = {}\n",
    "rand_r2_dict = {}\n",
    "\n",
    "#final dataframe that saves both dictionaries\n",
    "final_rand_data = pd.DataFrame()\n",
    "\n",
    "#for each batch, run the simulations and save the results\n",
    "for batch in batch_sizes:\n",
    "\n",
    "    #getting runtime\n",
    "    start = time.time()\n",
    "    #run simulation\n",
    "    rand_rmse, rand_r2, final_inds = random_sampling(batch)\n",
    "    #get ending runtime\n",
    "    end = time.time()\n",
    "\n",
    "    #Saving runtime (divided by 3 to get per-seed results)\n",
    "    final_rand_data[f\"{batch}_runtime\"] = (end - start) / 3\n",
    "    \n",
    "    #calculating mean and std dev of RMSE across the 3 seeds\n",
    "    rand_rmse_mean = np.mean(rand_rmse, axis=0)\n",
    "    rand_rmse_stdev = np.std(rand_rmse, axis=0)\n",
    "    print(\"mean:\", rand_rmse_mean)\n",
    "    print(\"standard deviation:\", rand_rmse_stdev)\n",
    "    #saving data to the dictionary\n",
    "    rand_rmse_dict[batch] = (rand_rmse, rand_rmse_mean, rand_rmse_stdev)\n",
    "    #adding it to a new column in the dataframe for the specific batch\n",
    "    final_rand_data[f\"{batch}_rmse_mean\"] = rand_rmse_mean\n",
    "    final_rand_data[f\"{batch}_rmse_stdev\"] = rand_rmse_stdev\n",
    "\n",
    "    #calculating mean and std dev of R^2 across the 3 seeds\n",
    "    rand_r2_mean = np.mean(rand_r2, axis=0)\n",
    "    rand_r2_stdev = np.std(rand_r2, axis=0)\n",
    "    print(\"mean:\", rand_r2_mean)\n",
    "    print(\"standard deviation:\", rand_r2_stdev)\n",
    "    #saving data to dictionary\n",
    "    rand_r2_dict[batch] = (rand_r2, rand_r2_mean, rand_r2_stdev)\n",
    "    #adding it to a new column in the dataframe for the specific abtch\n",
    "    final_rand_data[f\"{batch}_r2_mean\"] = rand_r2_mean\n",
    "    final_rand_data[f\"{batch}_r2_stdev\"] = rand_r2_stdev\n",
    "\n",
    "    #saving indices into a csv file for the specific batch\n",
    "    final_inds = pd.Series(final_inds)\n",
    "    final_inds.to_csv(f\"{batch}_final_inds_random.csv\")\n",
    "\n",
    "#writing final dataframe to a csv\n",
    "final_rand_data.to_csv(\"final_data_random.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainity Sampling\n",
    "## - Mean, Variance, UCB, Biased UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining cells run the above 4 uncertainty sampling methods, starting with 1,000 ligands and stopping once 1,500 ligands have been added\n",
    "to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function runs a random forest classifier on the training datafor the BIASED UCB method. \n",
    "#It returns the probability of being active/inactive for all of the unseen datapoints\n",
    "\n",
    "def active_prediction(start_X, classes, remaining_X, seed):\n",
    "\n",
    "    #creating random forest classifier\n",
    "    m = RandomForestClassifier(n_estimators=20, random_state=seed)\n",
    "    #fitting to training data\n",
    "    m.fit(start_X, classes.ravel())\n",
    "    #predicting on unseen data\n",
    "    probs = m.predict_proba(remaining_X)\n",
    "    #returning probabilities\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function runs sequential model-based optimization. This function works by training a GP model on the start data, \n",
    "#and return the GP outputs (mean vector and variance vector) of all unseen data\n",
    "def smbo(start_X, start_y, remaining_X):\n",
    "    \n",
    "    #creating a linear GP kernel based on the number of columns in the dataframe (in this case 1)\n",
    "    k = GPy.kern.Linear(len(start_X[0]))\n",
    "    #fitting GP regression model to the start data \n",
    "    m = GPy.models.GPRegression(start_X, start_y, k)\n",
    "    #predict GP mean and variance of the unseen data\n",
    "    mean, var = m.predict(remaining_X, full_cov=False)\n",
    "    #return the GP mean and variance of the unseen data\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function runs uncertainty sampling based on the batch size (using the batch_size parameter)\n",
    "#for each seed, we randomly start with 1,000 ligands to train on. While the training set isn't 1,500 ligands yet,\n",
    "#we get the performance (RMSE, R^2) if the counter is at a multiple of 50,\n",
    "#and then append new batch size set of data to the training data based on one of 4 query seleciton strategies (using the flag parameter):\n",
    "#   - Mean, Variance, UCB, Biased UCB\n",
    "#At the end of the seed, we append the list of RMSE and R^2 values to the master list\n",
    "def uncertainty_sampling(flag, batch_size):\n",
    "    #testing for 3 seeds\n",
    "    seeds = [1,2,3]\n",
    "\n",
    "    #master list of RMSE and R^2 values\n",
    "    rmse_vals = []\n",
    "    r2_vals = []\n",
    "\n",
    "    #for each seed...\n",
    "    for seed in seeds:\n",
    "\n",
    "        #list of selected indices by query selection method\n",
    "        final_inds = []\n",
    "\n",
    "        #current seed's list of RMSE and R^2 values\n",
    "        rmse_instance =[]\n",
    "        r2_instance = []\n",
    "      \n",
    "        #dropping unecessary columns\n",
    "        working_data = data.drop(columns=\"smiles\")\n",
    "        #setting starting size to 1,000\n",
    "        size = 1000\n",
    "        #randomly selecting the starting 1,000 ligands\n",
    "        start_data = working_data.sample(n=size, random_state=seed)\n",
    "        \n",
    "        #subsetting the starting data: X, ligands, and classes (used for biased UCB method)\n",
    "        start_X = np.array([val for val in start_data['fingerprint'].values])\n",
    "        start_y = start_data['target'].values.reshape(-1,1)\n",
    "        start_classes = start_data['active'].values.reshape(-1,1)\n",
    "\n",
    "        #subsetting the remaining unseen data: X and ligands\n",
    "        remaining_data = working_data.drop(start_data.index)\n",
    "        remaining_X = np.array([val for val in remaining_data['fingerprint'].values])\n",
    "        remaining_y = remaining_data['target'].values.reshape(-1,1)\n",
    "\n",
    "        #set initial variables for calculating UCB\n",
    "        Dsize = len(start_X) #size of the dataset\n",
    "        bo_lambda = 0.1 #how much importance to place on variance in UCB\n",
    "        bo_iters = 1 #number of iterations\n",
    "\n",
    "        #calculate beta constant from the course-provided material\n",
    "        beta = 2 * math.log(Dsize * math.pow(bo_iters,2) * math.pow(np.pi,2) / (6 * bo_lambda) )\n",
    "\n",
    "        #getting initial counter\n",
    "        counter = 0\n",
    "\n",
    "        #Until we sample another 500 ligands...\n",
    "        while len(start_X) <= 1500:\n",
    "            \n",
    "            #calculate RMSE adn R^2 evry 50 ligands\n",
    "            if counter % 50 == 0:\n",
    "                #creating linear kernel\n",
    "                k = GPy.kern.Linear(len(start_X[0]))\n",
    "\n",
    "                #training and optimizing GP regression model\n",
    "                m = GPy.models.GPRegression(start_X, start_y, k)\n",
    "\n",
    "                #Predicting on unseen data\n",
    "                pred_means, pred_vars = m.predict(remaining_X)\n",
    "\n",
    "                #Getting rmse and R^2 score\n",
    "                rmse_instance.append(np.sqrt(mean_squared_error(remaining_y, pred_means)))\n",
    "                r2_instance.append(r2_score(remaining_y, pred_means))\n",
    "            \n",
    "            #once we get to 1,500 ligands, exit the current seed\n",
    "            if len(start_X) >= 1500:\n",
    "                break\n",
    "\n",
    "            #run sequential model-based optimization and get the GP parameters to select the next instance\n",
    "            mean, var = smbo(start_X, start_y, remaining_X)\n",
    "\n",
    "            #depending on the selection function, we calculate a specific alpha_full value\n",
    "            if flag == \"ucb\":\n",
    "                #get the UCB value at each x: (mean + beta*var)\n",
    "                alpha_full = mean + math.sqrt(beta) * var\n",
    "                alpha_full = [item for sublist in alpha_full for item in sublist]\n",
    "\n",
    "            elif flag == \"biased\":\n",
    "                #get the biased UCB value at each x: (mean + beta*var) * prob_active\n",
    "\n",
    "                #get the probability of each unseen data being active\n",
    "                probs = active_prediction(start_X, start_classes, remaining_X, seed)\n",
    "                probs = [prob[1] for prob in probs]\n",
    "\n",
    "                #calculate base ucb\n",
    "                alpha_full = mean + math.sqrt(beta) * var\n",
    "                alpha_full = [item for sublist in alpha_full for item in sublist]\n",
    "\n",
    "                #multiply by prob_active\n",
    "                alpha_full = np.array(alpha_full) * probs\n",
    "                alpha_full = alpha_full.tolist()\n",
    "\n",
    "            elif flag == \"mean\":\n",
    "                #just save the predicted mean value\n",
    "                alpha_full = mean\n",
    "                alpha_full = [item for sublist in alpha_full for item in sublist]\n",
    "            else:\n",
    "                # just save the predicted variance calue\n",
    "                alpha_full = [item for sublist in var for item in sublist]\n",
    "    \n",
    "            #get the index for the row with the largest UCB/biased UCB/mean/var by sorting the list and maintaining the correct index\n",
    "            sorted_rows = sorted(range(len(alpha_full)), key=lambda x: alpha_full[x], reverse=True)\n",
    "            #getting indices of selected data\n",
    "            inds = sorted_rows[:batch_size]\n",
    "\n",
    "            #extracting the new data rows to add\n",
    "            rows_to_add = remaining_data.iloc[inds]\n",
    "            #saving the indices to the list for UMAP analysis\n",
    "            for idx in rows_to_add.index:\n",
    "                final_inds.append(idx)\n",
    "\n",
    "            #adding new row(s) to the starting data and re-separating X, ligands, and classes\n",
    "            start_data = pd.concat([start_data, rows_to_add])\n",
    "            start_X = np.array([val for val in start_data['fingerprint'].values])\n",
    "            start_y = start_data['target'].values.reshape(-1,1)\n",
    "            start_classes = start_data['active'].values.reshape(-1,1)\n",
    "\n",
    "            #removing row(s) from the remaining data and re-separating X, ligands\n",
    "            remaining_data = working_data.drop(start_data.index)\n",
    "            remaining_X = np.array([val for val in remaining_data['fingerprint'].values])\n",
    "            remaining_y = remaining_data['target'].values.reshape(-1,1)\n",
    "\n",
    "            #increment counter\n",
    "            counter += batch_size\n",
    "\n",
    "        #appending the current seed's values to the master list\n",
    "        rmse_vals.append(rmse_instance)\n",
    "        r2_vals.append(r2_instance)\n",
    "        \n",
    "    #returning the master lists and the indices (just from seed 3)\n",
    "    return rmse_vals, r2_vals, final_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell runs the UCB simulations for varying batch sizes (1, 25, and 50), and saves the RMSE and R^2 values, as well as the runtimes\n",
    "#The results are saved into a dictionary for each batch and then stored into a dataframe for exporting\n",
    "\n",
    "#list of batch sizes\n",
    "batch_sizes = [1, 25, 50]\n",
    "flag = \"ucb\"\n",
    "\n",
    "#dictionaries for RMSE and R^2 values\n",
    "ucb_rmse_dict = {}\n",
    "ucb_r2_dict = {}\n",
    "\n",
    "#final dataframe storing all scores and runtimes\n",
    "final_ucb_data = pd.DataFrame()\n",
    "\n",
    "#for each batch\n",
    "for batch in batch_sizes:\n",
    "\n",
    "    #getting start time\n",
    "    start = time.time()\n",
    "    #running UCB\n",
    "    ucb_rmse, ucb_r2, final_inds = uncertainty_sampling(flag, batch)\n",
    "    #getting ending time\n",
    "    end = time.time()\n",
    "\n",
    "    #append runtime to dataframe\n",
    "    final_ucb_data[f\"{batch}_runtime\"] = end - start\n",
    "\n",
    "    #calculate the mean and std. dev of RMSE across 3 seeds\n",
    "    ucb_rmse_mean = np.mean(ucb_rmse, axis=0)\n",
    "    ucb_rmse_stdev = np.std(ucb_rmse, axis=0)\n",
    "    print(\"mean:\", ucb_rmse_mean)\n",
    "    print(\"standard deviation:\", ucb_rmse_stdev)\n",
    "    #add results to dictionary\n",
    "    ucb_rmse_dict[batch] = (ucb_rmse, ucb_rmse_mean, ucb_rmse_stdev)\n",
    "    \n",
    "    #add dictionary to new columns in dataframe\n",
    "    final_ucb_data[f\"{batch}_rmse_mean\"] = ucb_rmse_mean\n",
    "    final_ucb_data[f\"{batch}_rmse_stdev\"] = ucb_rmse_stdev\n",
    "\n",
    "    #calculate the mean and std. dev of R^2 across 3 seeds\n",
    "    ucb_r2_mean = np.mean(ucb_r2, axis=0)\n",
    "    ucb_r2_stdev = np.std(ucb_r2, axis=0)\n",
    "    print(\"mean:\", ucb_r2_mean)\n",
    "    print(\"standard deviation:\", ucb_r2_stdev)\n",
    "    #add results to dictionary\n",
    "    ucb_r2_dict[batch] = (ucb_r2, ucb_r2_mean, ucb_r2_stdev)\n",
    "    #add dictionaries to new columns in dataframe\n",
    "    final_ucb_data[f\"{batch}_r2_mean\"] = ucb_r2_mean\n",
    "    final_ucb_data[f\"{batch}_r2_stdev\"] = ucb_r2_stdev\n",
    "\n",
    "    #saving selected indices to a csv\n",
    "    final_inds = pd.Series(final_inds)\n",
    "    final_inds.to_csv(f\"{batch}_final_inds_ucb.csv\")\n",
    "\n",
    "#writing final dataframe\n",
    "final_ucb_data.to_csv(\"final_data_ucb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell runs the biased UCB simulations for varying batch sizes (1, 25, and 50), and saves the RMSE and R^2 values, as well as the runtimes\n",
    "#The results are saved into a dictionary for each batch and then stored into a dataframe for exporting\n",
    "\n",
    "#list of batch sizes\n",
    "batch_sizes = [1, 25, 50]\n",
    "flag = \"biased\"\n",
    "\n",
    "#dictionaries storing RMSE and R^2 values\n",
    "ucb_rmse_dict = {}\n",
    "ucb_r2_dict = {}\n",
    "\n",
    "#final dataframe to store results\n",
    "final_ucb_biased_data = pd.DataFrame()\n",
    "\n",
    "#for each batch size\n",
    "for batch in batch_sizes:\n",
    "\n",
    "    #getting start time\n",
    "    start = time.time()\n",
    "    #running biased UCB seleciton\n",
    "    ucb_rmse, ucb_r2, final_inds = uncertainty_sampling(flag, batch)\n",
    "    #getting end time\n",
    "    end = time.time()\n",
    "\n",
    "    #appending runtime to dataframe\n",
    "    final_ucb_biased_data[f\"{batch}_runtime\"] = end - start\n",
    "\n",
    "    #calculating mean and std. dev of RMSE across seeds\n",
    "    ucb_rmse_mean = np.mean(ucb_rmse, axis=0)\n",
    "    ucb_rmse_stdev = np.std(ucb_rmse, axis=0)\n",
    "    print(\"mean:\", ucb_rmse_mean)\n",
    "    print(\"standard deviation:\", ucb_rmse_stdev)\n",
    "    #adding results to dictionary\n",
    "    ucb_rmse_dict[batch] = (ucb_rmse, ucb_rmse_mean, ucb_rmse_stdev)\n",
    "    #saving data to final dataframe\n",
    "    final_ucb_biased_data[f\"{batch}_rmse_mean\"] = ucb_rmse_mean\n",
    "    final_ucb_biased_data[f\"{batch}_rmse_stdev\"] = ucb_rmse_stdev\n",
    "\n",
    "    #calculating mean and std. dev of R^2 across seeds\n",
    "    ucb_r2_mean = np.mean(ucb_r2, axis=0)\n",
    "    ucb_r2_stdev = np.std(ucb_r2, axis=0)\n",
    "    print(\"mean:\", ucb_r2_mean)\n",
    "    print(\"standard deviation:\", ucb_r2_stdev)\n",
    "    #adding results to dictionary\n",
    "    ucb_r2_dict[batch] = (ucb_r2, ucb_r2_mean, ucb_r2_stdev)\n",
    "    #savind data to final dataframe\n",
    "    final_ucb_biased_data[f\"{batch}_r2_mean\"] = ucb_r2_mean\n",
    "    final_ucb_biased_data[f\"{batch}_r2_stdev\"] = ucb_r2_stdev\n",
    "\n",
    "    #save selected indices\n",
    "    final_inds = pd.Series(final_inds)\n",
    "    final_inds.to_csv(f\"{batch}_final_inds_ucb_biased.csv\")\n",
    "\n",
    "#save final dataframe\n",
    "final_ucb_biased_data.to_csv(\"final_data_ucb_biased.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell runs the mean simulations for varying batch sizes (1, 25, and 50), and saves the RMSE and R^2 values, as well as the runtimes\n",
    "#The results are saved into a dictionary for each batch and then stored into a dataframe for exporting\n",
    "\n",
    "#batch sizes\n",
    "batch_sizes = [1, 25, 50]\n",
    "flag = \"mean\"\n",
    "#dictionary to store RMSE adn R^2 values\n",
    "mean_rmse_dict = {}\n",
    "mean_r2_dict = {}\n",
    "#final dataframe with results\n",
    "final_mean_data = pd.DataFrame()\n",
    "\n",
    "for batch in batch_sizes:\n",
    "    #get start time\n",
    "    start = time.time()\n",
    "    #run mean simulation\n",
    "    mean_rmse, mean_r2, final_inds = uncertainty_sampling(flag, batch)\n",
    "    #get finish time\n",
    "    end = time.time()\n",
    "    #save runtime to dataframe\n",
    "    final_mean_data[f\"{batch}_runtime\"] = end - start\n",
    "    \n",
    "    #calculating mean and std. dev of RMSE across seeds\n",
    "    mean_rmse_mean = np.mean(mean_rmse, axis=0)\n",
    "    mean_rmse_stdev = np.std(mean_rmse, axis=0)\n",
    "    print(\"mean:\", mean_rmse_mean)\n",
    "    print(\"standard deviation:\", mean_rmse_stdev)\n",
    "    #saving results to dictionary\n",
    "    mean_rmse_dict[batch] = (mean_rmse, mean_rmse_mean, mean_rmse_stdev)\n",
    "    #adding dictionary as new columns to dataframe\n",
    "    final_mean_data[f\"{batch}_rmse_mean\"] = mean_rmse_mean\n",
    "    final_mean_data[f\"{batch}_rmse_stdev\"] = mean_rmse_stdev\n",
    "\n",
    "    #calculating mean and std. dev of R^2 across seeds\n",
    "    mean_r2_mean = np.mean(mean_r2, axis=0)\n",
    "    mean_r2_stdev = np.std(mean_r2, axis=0)\n",
    "    print(\"mean:\", mean_r2_mean)\n",
    "    print(\"standard deviation:\", mean_r2_stdev)\n",
    "    #saving results to dictionary\n",
    "    mean_r2_dict[batch] = (mean_r2, mean_r2_mean, mean_r2_stdev)\n",
    "    #adding dictionary as new columns to dataframe\n",
    "    final_mean_data[f\"{batch}_r2_mean\"] = mean_r2_mean\n",
    "    final_mean_data[f\"{batch}_r2_stdev\"] = mean_r2_stdev\n",
    "\n",
    "    #saving final selected indices\n",
    "    final_inds = pd.Series(final_inds)\n",
    "    final_inds.to_csv(f\"{batch}_final_inds_mean.csv\")\n",
    "\n",
    "#save final dataframe\n",
    "final_mean_data.to_csv(\"final_data_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell runs the variance simulations for varying batch sizes (1, 25, and 50), and saves the RMSE and R^2 values, as well as the runtimes\n",
    "#The results are saved into a dictionary for each batch and then stored in a dataframe for exporting\n",
    "\n",
    "#batch sizes\n",
    "batch_sizes = [1, 25, 50]\n",
    "flag = \"var\"\n",
    "#dictionaries storing RMSE and R^2 values\n",
    "var_rmse_dict = {}\n",
    "var_r2_dict = {}\n",
    "\n",
    "#final dataframe storing value and runtimes\n",
    "final_var_data = pd.DataFrame()\n",
    "\n",
    "#for each batch...\n",
    "for batch in batch_sizes:\n",
    "    \n",
    "    #get start time\n",
    "    start = time.time()\n",
    "    #run variance simulations\n",
    "    var_rmse, var_r2, final_inds = uncertainty_sampling(flag, batch)\n",
    "    #get end time\n",
    "    end = time.time()\n",
    "    #adding run time to dataframe\n",
    "    final_var_data[f\"{batch}_runtime\"] = end - start\n",
    "\n",
    "    #getting mean and std. dev of RMSE across seeds\n",
    "    var_rmse_mean = np.mean(var_rmse, axis=0)\n",
    "    var_rmse_stdev = np.std(var_rmse, axis=0)\n",
    "    print(\"mean:\", var_rmse_mean)\n",
    "    print(\"standard deviation:\", var_rmse_stdev)\n",
    "    #adding values to dictionary \n",
    "    var_rmse_dict[batch] = (var_rmse, var_rmse_mean, var_rmse_stdev)\n",
    "    #adding dictionary to dataframe as new columns\n",
    "    final_var_data[f\"{batch}_rmse_mean\"] = var_rmse_mean\n",
    "    final_var_data[f\"{batch}_rmse_stdev\"] = var_rmse_stdev\n",
    "\n",
    "    #getting mean and std. dev of R^2 across seeds\n",
    "    var_r2_mean = np.mean(var_r2, axis=0)\n",
    "    var_r2_stdev = np.std(var_r2, axis=0)\n",
    "    print(\"mean:\", var_r2_mean)\n",
    "    print(\"standard deviation:\", var_r2_stdev)\n",
    "    #adding values to dictionary\n",
    "    var_r2_dict[batch] = (var_r2, var_r2_mean, var_r2_stdev)\n",
    "    #adding dictionary to dataframe as new columns\n",
    "    final_var_data[f\"{batch}_r2_mean\"] = var_r2_mean\n",
    "    final_var_data[f\"{batch}_r2_stdev\"] = var_r2_stdev\n",
    "\n",
    "    #saving final selected indices per batch\n",
    "    final_inds = pd.Series(final_inds)\n",
    "    final_inds.to_csv(f\"{batch}_final_inds_var.csv\")\n",
    "\n",
    "#saving final dataframe of results\n",
    "final_var_data.to_csv(\"final_data_var.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "70d57c135da11913b2ad31fa6150ab201732e694e7c1a956b2909783da6a3273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
